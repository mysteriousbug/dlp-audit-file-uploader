import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from collections import Counter
import warnings
warnings.filterwarnings('ignore')

def perform_positive_timediff_rca(sample_file_path, original_incidents_df=None):
    """
    Perform root cause analysis on sampled records with POSITIVE time differences
    between incident creation and outage end times.
    
    Parameters:
    -----------
    sample_file_path : str
        Path to Excel file containing sampled records (with positive time differences)
    original_incidents_df : pandas.DataFrame, optional
        Original incidents dataframe for additional context (category, priority, etc.)
        
    Returns:
    --------
    dict
        Dictionary containing RCA findings and metrics
    """
    
    rca_results = {
        'patterns': {},
        'root_causes': [],
        'recommendations': [],
        'time_buckets': {},
        'detailed_analysis': []
    }
    
    print("=" * 80)
    print("ROOT CAUSE ANALYSIS - POSITIVE TIME DIFFERENCES")
    print("=" * 80)
    
    # Read the sample data
    try:
        df = pd.read_excel(sample_file_path)
        print(f"\n‚úÖ Successfully loaded {len(df)} sampled records")
    except Exception as e:
        print(f"‚ùå Error reading file: {e}")
        return None
    
    print(f"Columns: {list(df.columns)}\n")
    
    # Convert datetime columns
    df['sys_created_on_x_clean'] = pd.to_datetime(df['sys_created_on_x_clean'], errors='coerce')
    df['end_date_clean'] = pd.to_datetime(df['end_date_clean'], errors='coerce')
    
    # Verify all records have positive differences
    if 'total_hours' in df.columns:
        negative_count = (df['total_hours'] < 0).sum()
        zero_count = (df['total_hours'] == 0).sum()
        if negative_count > 0 or zero_count > 0:
            print(f"‚ö†Ô∏è  Warning: Found {negative_count} negative and {zero_count} zero differences")
            print("Filtering to only positive differences...")
            df = df[df['total_hours'] > 0].reset_index(drop=True)
            print(f"Remaining records: {len(df)}")
    
    # 1. SAMPLE OVERVIEW
    print("\n" + "=" * 80)
    print("1. SAMPLE OVERVIEW")
    print("=" * 80)
    
    print(f"\nTotal sampled records: {len(df)}")
    print(f"Date range: {df['sys_created_on_x_clean'].min()} to {df['sys_created_on_x_clean'].max()}")
    
    rca_results['patterns']['sample_info'] = {
        'total_samples': len(df),
        'date_range_start': str(df['sys_created_on_x_clean'].min()),
        'date_range_end': str(df['sys_created_on_x_clean'].max())
    }
    
    # 2. TIME DIFFERENCE DISTRIBUTION ANALYSIS
    print("\n" + "=" * 80)
    print("2. TIME DIFFERENCE DISTRIBUTION ANALYSIS")
    print("=" * 80)
    
    # Categorize by time ranges
    time_categories = pd.cut(df['total_hours'], 
                            bins=[0, 1, 4, 8, 24, 72, 168, float('inf')],
                            labels=['<1 hour', '1-4 hours', '4-8 hours', '8-24 hours', 
                                   '1-3 days', '3-7 days', '>7 days'])
    
    time_dist = time_categories.value_counts().sort_index()
    
    print("\nTime Difference Distribution:")
    for category, count in time_dist.items():
        pct = count / len(df) * 100
        print(f"  {category:15}: {count:3} records ({pct:5.1f}%)")
    
    rca_results['time_buckets'] = time_dist.to_dict()
    
    # Statistical summary
    print(f"\n--- Statistical Summary (hours) ---")
    print(f"Mean:   {df['total_hours'].mean():.2f} hours ({df['total_hours'].mean()/24:.2f} days)")
    print(f"Median: {df['total_hours'].median():.2f} hours ({df['total_hours'].median()/24:.2f} days)")
    print(f"Std:    {df['total_hours'].std():.2f} hours")
    print(f"Min:    {df['total_hours'].min():.2f} hours")
    print(f"Max:    {df['total_hours'].max():.2f} hours ({df['total_hours'].max()/24:.2f} days)")
    print(f"25th percentile: {df['total_hours'].quantile(0.25):.2f} hours")
    print(f"75th percentile: {df['total_hours'].quantile(0.75):.2f} hours")
    print(f"95th percentile: {df['total_hours'].quantile(0.95):.2f} hours")
    
    rca_results['patterns']['statistics'] = {
        'mean_hours': round(df['total_hours'].mean(), 2),
        'median_hours': round(df['total_hours'].median(), 2),
        'std_hours': round(df['total_hours'].std(), 2),
        'min_hours': round(df['total_hours'].min(), 2),
        'max_hours': round(df['total_hours'].max(), 2),
        'p25_hours': round(df['total_hours'].quantile(0.25), 2),
        'p75_hours': round(df['total_hours'].quantile(0.75), 2),
        'p95_hours': round(df['total_hours'].quantile(0.95), 2)
    }
    
    # 3. PATTERN IDENTIFICATION
    print("\n" + "=" * 80)
    print("3. PATTERN IDENTIFICATION & ROOT CAUSE ANALYSIS")
    print("=" * 80)
    
    # Pattern 1: Very short delays (<1 hour)
    very_short = df[df['total_hours'] < 1]
    if len(very_short) > 0:
        pct = len(very_short) / len(df) * 100
        print(f"\nüìä Pattern: Very Short Delays (<1 hour)")
        print(f"   Count: {len(very_short)} records ({pct:.1f}%)")
        print(f"   Average: {very_short['total_hours'].mean()*60:.1f} minutes")
        
        if pct >= 20:
            rca_results['root_causes'].append({
                'cause': 'Incidents logged very close to outage end time',
                'evidence': f"{len(very_short)} records ({pct:.1f}%) with <1 hour difference",
                'severity': 'Medium',
                'category': 'Timing/Process',
                'impact': 'May indicate reactive logging after issue resolution',
                'avg_delay': f"{very_short['total_hours'].mean()*60:.1f} minutes"
            })
    
    # Pattern 2: Short delays (1-4 hours)
    short = df[(df['total_hours'] >= 1) & (df['total_hours'] < 4)]
    if len(short) > 0:
        pct = len(short) / len(df) * 100
        print(f"\nüìä Pattern: Short Delays (1-4 hours)")
        print(f"   Count: {len(short)} records ({pct:.1f}%)")
        print(f"   Average: {short['total_hours'].mean():.2f} hours")
        
        if pct >= 25:
            rca_results['root_causes'].append({
                'cause': 'Moderate delay between outage end and incident logging',
                'evidence': f"{len(short)} records ({pct:.1f}%) with 1-4 hour difference",
                'severity': 'Medium',
                'category': 'Timing/Process',
                'impact': 'Incidents may be logged after troubleshooting/resolution',
                'avg_delay': f"{short['total_hours'].mean():.2f} hours"
            })
    
    # Pattern 3: Business hours delay (4-24 hours)
    business = df[(df['total_hours'] >= 4) & (df['total_hours'] < 24)]
    if len(business) > 0:
        pct = len(business) / len(df) * 100
        print(f"\nüìä Pattern: Business Hours Delay (4-24 hours)")
        print(f"   Count: {len(business)} records ({pct:.1f}%)")
        print(f"   Average: {business['total_hours'].mean():.2f} hours")
        
        if pct >= 20:
            rca_results['root_causes'].append({
                'cause': 'Significant delay suggesting next-day logging',
                'evidence': f"{len(business)} records ({pct:.1f}%) with 4-24 hour difference",
                'severity': 'High',
                'category': 'Process Gap',
                'impact': 'Could indicate incidents logged in next business day/shift',
                'avg_delay': f"{business['total_hours'].mean():.2f} hours"
            })
    
    # Pattern 4: Multi-day delays (1-7 days)
    multi_day = df[(df['total_hours'] >= 24) & (df['total_hours'] < 168)]
    if len(multi_day) > 0:
        pct = len(multi_day) / len(df) * 100
        avg_days = multi_day['total_hours'].mean() / 24
        print(f"\nüìä Pattern: Multi-Day Delays (1-7 days)")
        print(f"   Count: {len(multi_day)} records ({pct:.1f}%)")
        print(f"   Average: {avg_days:.1f} days")
        
        if pct >= 15:
            rca_results['root_causes'].append({
                'cause': 'Substantial delay in incident documentation',
                'evidence': f"{len(multi_day)} records ({pct:.1f}%) with 1-7 day difference",
                'severity': 'High',
                'category': 'Compliance/Process',
                'impact': 'Major delays in formal incident logging after resolution',
                'avg_delay': f"{avg_days:.1f} days"
            })
    
    # Pattern 5: Extended delays (>7 days)
    extended = df[df['total_hours'] >= 168]
    if len(extended) > 0:
        pct = len(extended) / len(df) * 100
        avg_days = extended['total_hours'].mean() / 24
        print(f"\nüìä Pattern: Extended Delays (>7 days)")
        print(f"   Count: {len(extended)} records ({pct:.1f}%)")
        print(f"   Average: {avg_days:.1f} days")
        print(f"   Max: {extended['total_hours'].max()/24:.1f} days")
        
        if len(extended) > 0:
            rca_results['root_causes'].append({
                'cause': 'Critical delay in incident documentation',
                'evidence': f"{len(extended)} records ({pct:.1f}%) with >7 day difference",
                'severity': 'Critical',
                'category': 'Compliance/Audit Risk',
                'impact': 'Severe delays may indicate missing audit trail or retrospective logging',
                'avg_delay': f"{avg_days:.1f} days",
                'max_delay': f"{extended['total_hours'].max()/24:.1f} days"
            })
    
    # 4. TEMPORAL PATTERNS
    print("\n" + "=" * 80)
    print("4. TEMPORAL PATTERNS ANALYSIS")
    print("=" * 80)
    
    # Extract time components
    df['incident_hour'] = df['sys_created_on_x_clean'].dt.hour
    df['incident_day_of_week'] = df['sys_created_on_x_clean'].dt.day_name()
    df['incident_month'] = df['sys_created_on_x_clean'].dt.month_name()
    
    # Hour of day analysis
    print("\n--- Incident Creation by Hour of Day ---")
    hour_dist = df['incident_hour'].value_counts().sort_index()
    
    business_hours = df[(df['incident_hour'] >= 9) & (df['incident_hour'] <= 17)]
    after_hours = df[(df['incident_hour'] < 9) | (df['incident_hour'] > 17)]
    
    print(f"Business hours (9 AM - 5 PM): {len(business_hours)} records ({len(business_hours)/len(df)*100:.1f}%)")
    print(f"After hours: {len(after_hours)} records ({len(after_hours)/len(df)*100:.1f}%)")
    
    rca_results['patterns']['temporal'] = {
        'business_hours_pct': round(len(business_hours)/len(df)*100, 1),
        'after_hours_pct': round(len(after_hours)/len(df)*100, 1)
    }
    
    # Day of week analysis
    print("\n--- Incident Creation by Day of Week ---")
    dow_dist = df['incident_day_of_week'].value_counts()
    for day, count in dow_dist.items():
        print(f"  {day:10}: {count:3} records ({count/len(df)*100:5.1f}%)")
    
    # 5. OUTLIER ANALYSIS
    print("\n" + "=" * 80)
    print("5. OUTLIER ANALYSIS")
    print("=" * 80)
    
    # Using IQR method
    Q1 = df['total_hours'].quantile(0.25)
    Q3 = df['total_hours'].quantile(0.75)
    IQR = Q3 - Q1
    outlier_threshold = Q3 + 1.5 * IQR
    
    outliers = df[df['total_hours'] > outlier_threshold]
    
    print(f"\nOutlier threshold (Q3 + 1.5*IQR): {outlier_threshold:.2f} hours ({outlier_threshold/24:.1f} days)")
    print(f"Number of outliers: {len(outliers)} ({len(outliers)/len(df)*100:.1f}%)")
    
    if len(outliers) > 0:
        print(f"Average outlier delay: {outliers['total_hours'].mean():.2f} hours ({outliers['total_hours'].mean()/24:.1f} days)")
        print(f"Largest delay: {outliers['total_hours'].max():.2f} hours ({outliers['total_hours'].max()/24:.1f} days)")
        
        rca_results['patterns']['outliers'] = {
            'count': len(outliers),
            'percentage': round(len(outliers)/len(df)*100, 1),
            'avg_hours': round(outliers['total_hours'].mean(), 2),
            'max_hours': round(outliers['total_hours'].max(), 2),
            'threshold_hours': round(outlier_threshold, 2)
        }
        
        # Show top 5 outliers
        print("\n--- Top 5 Extreme Delays ---")
        top_outliers = df.nlargest(5, 'total_hours')[['sys_created_on_x_original', 'end_date_original', 
                                                        'total_difference', 'days', 'hours', 'total_hours']]
        print(top_outliers.to_string(index=False))
    
    # 6. DETAILED RECORD ANALYSIS
    print("\n" + "=" * 80)
    print("6. DETAILED RECORD-BY-RECORD ANALYSIS")
    print("=" * 80)
    
    for idx, row in df.iterrows():
        hours = row['total_hours']
        
        # Classify each record
        if hours < 1:
            classification = 'Very Short Delay'
            likely_cause = 'Incident logged immediately after or during resolution'
            severity = 'Low'
        elif hours < 4:
            classification = 'Short Delay'
            likely_cause = 'Incident logged shortly after resolution, possible same-shift logging'
            severity = 'Low'
        elif hours < 8:
            classification = 'Moderate Delay'
            likely_cause = 'Incident logged after resolution, possibly different shift'
            severity = 'Medium'
        elif hours < 24:
            classification = 'Business Day Delay'
            likely_cause = 'Incident logged next business day after resolution'
            severity = 'Medium'
        elif hours < 72:
            classification = 'Multi-Day Delay'
            likely_cause = 'Significant delay, possible administrative backlog'
            severity = 'High'
        elif hours < 168:
            classification = 'Week-Long Delay'
            likely_cause = 'Major delay, potential process gap or retrospective logging'
            severity = 'High'
        else:
            classification = 'Extended Delay'
            likely_cause = 'Critical delay, likely retrospective logging or data quality issue'
            severity = 'Critical'
        
        rca_results['detailed_analysis'].append({
            'record_index': idx + 1,
            'sys_created_on': str(row['sys_created_on_x_original']),
            'end_date': str(row['end_date_original']),
            'time_difference': row['total_difference'],
            'total_hours': round(hours, 2),
            'total_days': round(hours/24, 2),
            'classification': classification,
            'likely_cause': likely_cause,
            'severity': severity
        })
        
        if idx < 10:  # Print first 10
            print(f"\nRecord {idx + 1}:")
            print(f"  Time Difference: {row['total_difference']} ({hours:.2f} hours)")
            print(f"  Classification: {classification}")
            print(f"  Likely Cause: {likely_cause}")
            print(f"  Severity: {severity}")
    
    if len(df) > 10:
        print(f"\n... and {len(df) - 10} more records (see detailed export)")
    
    # 7. GENERATE RECOMMENDATIONS
    print("\n" + "=" * 80)
    print("7. ROOT CAUSE SUMMARY & RECOMMENDATIONS")
    print("=" * 80)
    
    print("\nIdentified Root Causes:")
    for i, rc in enumerate(rca_results['root_causes'], 1):
        print(f"\n{i}. [{rc['severity']}] {rc['cause']}")
        print(f"   Category: {rc['category']}")
        print(f"   Evidence: {rc['evidence']}")
        print(f"   Impact: {rc['impact']}")
        print(f"   Average Delay: {rc.get('avg_delay', 'N/A')}")
    
    # Generate recommendations
    recommendations = generate_recommendations(rca_results, df)
    rca_results['recommendations'] = recommendations
    
    print("\n" + "=" * 80)
    print("RECOMMENDATIONS (Prioritized)")
    print("=" * 80)
    
    for i, rec in enumerate(recommendations, 1):
        print(f"\n{rec['priority']} - Recommendation {i}:")
        print(f"  {rec['recommendation']}")
        print(f"  Details: {rec['details']}")
        print(f"  Expected Impact: {rec['impact']}")
    
    return rca_results


def generate_recommendations(rca_results, df):
    """Generate targeted recommendations based on findings."""
    recommendations = []
    
    avg_hours = rca_results['patterns']['statistics']['mean_hours']
    median_hours = rca_results['patterns']['statistics']['median_hours']
    
    # Recommendation based on overall delay
    if avg_hours > 24:
        recommendations.append({
            'priority': 'P1',
            'recommendation': 'Implement real-time incident logging requirement',
            'details': f'Average delay is {avg_hours:.1f} hours ({avg_hours/24:.1f} days). Establish policy that incidents must be logged within 1 hour of detection.',
            'impact': 'Reduce average logging delay by 60-80%, improve audit compliance'
        })
    elif avg_hours > 4:
        recommendations.append({
            'priority': 'P2',
            'recommendation': 'Establish same-shift incident logging policy',
            'details': f'Average delay is {avg_hours:.1f} hours. Require incidents to be logged during the same shift they occur.',
            'impact': 'Improve incident documentation timeliness'
        })
    
    # Recommendation for outliers
    if 'outliers' in rca_results['patterns']:
        outlier_count = rca_results['patterns']['outliers']['count']
        if outlier_count > 0:
            recommendations.append({
                'priority': 'P1',
                'recommendation': 'Investigate and remediate extreme delay cases',
                'details': f'{outlier_count} incidents have extreme delays (>{rca_results["patterns"]["outliers"]["threshold_hours"]:.1f} hours). Conduct individual review of these cases.',
                'impact': 'Identify systemic issues causing major delays'
            })
    
    # Recommendation based on root causes
    critical_causes = [rc for rc in rca_results['root_causes'] if rc['severity'] == 'Critical']
    high_causes = [rc for rc in rca_results['root_causes'] if rc['severity'] == 'High']
    
    if critical_causes:
        recommendations.append({
            'priority': 'P1',
            'recommendation': 'Address critical compliance gaps',
            'details': f'Found {len(critical_causes)} critical severity issues. Immediate action required for audit compliance.',
            'impact': 'Mitigate audit and compliance risks'
        })
    
    if high_causes:
        recommendations.append({
            'priority': 'P1',
            'recommendation': 'Implement automated incident creation from monitoring',
            'details': 'High delays suggest manual logging after resolution. Integrate monitoring tools to auto-create incidents.',
            'impact': 'Eliminate manual logging delays, improve accuracy'
        })
    
    # Process recommendations
    recommendations.append({
        'priority': 'P2',
        'recommendation': 'Establish incident logging SLA and monitoring',
        'details': 'Define maximum acceptable time between outage end and incident logging. Monitor compliance weekly.',
        'impact': 'Create accountability and visibility into logging practices'
    })
    
    recommendations.append({
        'priority': 'P2',
        'recommendation': 'Training on timely incident documentation',
        'details': 'Train teams on importance of real-time incident logging for audit trail and root cause analysis.',
        'impact': 'Improve team awareness and compliance'
    })
    
    # Data quality recommendation
    recommendations.append({
        'priority': 'P3',
        'recommendation': 'Implement data validation rules',
        'details': 'Add system validations to flag incidents where creation time is near/after outage end time.',
        'impact': 'Catch data quality issues at entry point'
    })
    
    return recommendations


def create_visualizations(df, rca_results):
    """Create comprehensive visualizations for the RCA."""
    
    fig = plt.figure(figsize=(16, 10))
    gs = fig.add_gridspec(3, 3, hspace=0.3, wspace=0.3)
    
    fig.suptitle('Positive Time Difference Root Cause Analysis', 
                 fontsize=16, fontweight='bold')
    
    # 1. Time Difference Distribution (Histogram)
    ax1 = fig.add_subplot(gs[0, :2])
    ax1.hist(df['total_hours'], bins=30, color='steelblue', edgecolor='black', alpha=0.7)
    ax1.axvline(df['total_hours'].mean(), color='red', linestyle='--', 
                label=f'Mean: {df["total_hours"].mean():.1f}h', linewidth=2)
    ax1.axvline(df['total_hours'].median(), color='green', linestyle='--', 
                label=f'Median: {df["total_hours"].median():.1f}h', linewidth=2)
    ax1.set_xlabel('Time Difference (hours)', fontsize=10)
    ax1.set_ylabel('Frequency', fontsize=10)
    ax1.set_title('Time Difference Distribution', fontweight='bold')
    ax1.legend()
    ax1.grid(True, alpha=0.3)
    
    # 2. Time Buckets (Bar Chart)
    ax2 = fig.add_subplot(gs[0, 2])
    time_buckets = pd.Series(rca_results['time_buckets'])
    colors = plt.cm.RdYlGn_r(np.linspace(0.2, 0.8, len(time_buckets)))
    bars = ax2.barh(range(len(time_buckets)), time_buckets.values, color=colors)
    ax2.set_yticks(range(len(time_buckets)))
    ax2.set_yticklabels(time_buckets.index, fontsize=8)
    ax2.set_xlabel('Count', fontsize=10)
    ax2.set_title('Time Buckets', fontweight='bold')
    ax2.grid(True, alpha=0.3, axis='x')
    
    for i, bar in enumerate(bars):
        width = bar.get_width()
        ax2.text(width, bar.get_y() + bar.get_height()/2, 
                f'{int(width)}', ha='left', va='center', fontsize=8)
    
    # 3. Box Plot
    ax3 = fig.add_subplot(gs[1, 0])
    bp = ax3.boxplot(df['total_hours'], vert=True, patch_artist=True)
    bp['boxes'][0].set_facecolor('lightblue')
    bp['boxes'][0].set_alpha(0.7)
    ax3.set_ylabel('Hours', fontsize=10)
    ax3.set_title('Time Difference Box Plot', fontweight='bold')
    ax3.grid(True, alpha=0.3, axis='y')
    
    # 4. Hour of Day Distribution
    ax4 = fig.add_subplot(gs[1, 1])
    hour_dist = df['incident_hour'].value_counts().sort_index()
    colors_hour = ['#ff6b6b' if (h < 9 or h > 17) else '#4ecdc4' for h in hour_dist.index]
    ax4.bar(hour_dist.index, hour_dist.values, color=colors_hour, alpha=0.7)
    ax4.set_xlabel('Hour of Day', fontsize=10)
    ax4.set_ylabel('Count', fontsize=10)
    ax4.set_title('Incidents by Hour of Day', fontweight='bold')
    ax4.axvspan(9, 17, alpha=0.2, color='green', label='Business Hours')
    ax4.legend(fontsize=8)
    ax4.grid(True, alpha=0.3, axis='y')
    
    # 5. Day of Week Distribution
    ax5 = fig.add_subplot(gs[1, 2])
    dow_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']
    dow_dist = df['incident_day_of_week'].value_counts().reindex(dow_order, fill_value=0)
    colors_dow = ['#4ecdc4' if day in ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday'] 
                  else '#ff6b6b' for day in dow_dist.index]
    bars = ax5.bar(range(len(dow_dist)), dow_dist.values, color=colors_dow, alpha=0.7)
    ax5.set_xticks(range(len(dow_dist)))
    ax5.set_xticklabels([d[:3] for d in dow_dist.index], fontsize=9)
    ax5.set_ylabel('Count', fontsize=10)
    ax5.set_title('Incidents by Day of Week', fontweight='bold')
    ax5.grid(True, alpha=0.3, axis='y')
    
    for bar in bars:
        height = bar.get_height()
        if height > 0:
            ax5.text(bar.get_x() + bar.get_width()/2., height,
                    f'{int(height)}', ha='center', va='bottom', fontsize=8)
    
    # 6. Cumulative Distribution
    ax6 = fig.add_subplot(gs[2, 0])
    sorted_hours = np.sort(df['total_hours'])
    cumulative = np.arange(1, len(sorted_hours) + 1) / len(sorted_hours) * 100
    ax6.plot(sorted_hours, cumulative, linewidth=2, color='purple')
    ax6.set_xlabel('Time Difference (hours)', fontsize=10)
    ax6.set_ylabel('Cumulative %', fontsize=10)
    ax6.set_title('Cumulative Distribution', fontweight='bold')
    ax6.grid(True, alpha=0.3)
    ax6.axhline(y=50, color='red', linestyle='--', alpha=0.5, label='50th percentile')
    ax6.axhline(y=95, color='orange', linestyle='--', alpha=0.5, label='95th percentile')
    ax6.legend(fontsize=8)
    
    # 7. Severity Distribution
    ax7 = fig.add_subplot(gs[2, 1])
    severity_counts = Counter([analysis['severity'] for analysis in rca_results['detailed_analysis']])
    severity_order = ['Low', 'Medium', 'High', 'Critical']
    severity_values = [severity_counts.get(s, 0) for s in severity_order]
    colors_sev = ['#44ff44', '#ffaa44', '#ff6b6b', '#8B0000']
    
    bars = ax7.bar(severity_order, severity_values, color=colors_sev, alpha=0.7, edgecolor='black')
    ax7.set_ylabel('Count', fontsize=10)
    ax7.set_title('Delay Severity Classification', fontweight='bold')
    ax7.grid(True, alpha=0.3, axis='y')
    
    for bar in bars:
        height = bar.get_height()
        if height > 0:
            ax7.text(bar.get_x() + bar.get_width()/2., height,
                    f'{int(height)}', ha='center', va='bottom', fontsize=9, fontweight='bold')
    
    # 8. Root Cause Categories
    ax8 = fig.add_subplot(gs[2, 2])
    if rca_results['root_causes']:
        rc_categories = [rc['category'] for rc in rca_results['root_causes']]
        rc_counts = Counter(rc_categories)
        colors_rc = plt.cm.Set3(range(len(rc_counts)))
        
        wedges, texts, autotexts = ax8.pie(rc_counts.values(), labels=rc_counts.keys(), 
                                            autopct='%1.0f%%', colors=colors_rc, startangle=90)
        for autotext in autotexts:
            autotext.set_fontsize(8)
            autotext.set_fontweight('bold')
        ax8.set_title('Root Cause Categories', fontweight='bold')
    else:
        ax8.text(0.5, 0.5, 'No root causes identified', 
                ha='center', va='center', transform=ax8.transAxes)
        ax8.set_title('Root Cause Categories', fontweight='bold')
    
    plt.savefig('positive_timediff_rca_analysis.png', dpi=300, bbox_inches='tight')
    print("\n‚úì Visualization saved as 'positive_timediff_rca_analysis.png'")
    plt.show()


def export_rca_report(rca_results, df, filename='positive_timediff_rca_report.txt'):
    """Export detailed RCA report to text file."""
    
    with open(filename, 'w') as f:
        f.write("=" * 100 + "\n")
        f.write("POSITIVE TIME DIFFERENCE ROOT CAUSE ANALYSIS REPORT\n")
        f.write("=" * 100 + "\n\n")
        f.write(f"Analysis Date: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write(f"Sample Size: {len(df)} records\n")
        f.write(f"Date Range: {rca_results['patterns']['sample_info']['date_range_start']} to ")
        f.write(f"{rca_results['patterns']['sample_info']['date_range_end']}\n\n")
        
        # Executive Summary
        f.write("\n" + "=" * 100 + "\n")
        f.write("EXECUTIVE SUMMARY\n")
        f.write("=" * 100 + "\n")
        stats = rca_results['patterns']['statistics']
        f.write(f"Average Time Difference: {stats['mean_hours']:.2f} hours ({stats['mean_hours']/24:.2f} days)\n")
        f.write(f"Median Time Difference: {stats['median_hours']:.2f} hours ({stats['median_hours']/24:.2f} days)\n")
        f.write(f"Range: {stats['min_hours']:.2f} to {stats['max_hours']:.2f} hours\n")
        f.write(f"Total Root Causes Identified: {len(rca_results['root_causes'])}\n")
        f.write(f"Critical Severity Issues: {sum(1 for rc in rca_results['root_causes'] if rc['severity'] == 'Critical')}\n")
        f.write(f"High Severity Issues: {sum(1 for rc in rca_results['root_causes'] if rc['severity'] == 'High')}\n")
        f.write(f"Total Recommendations: {len(rca_results['recommendations'])}\n\n")
        
        # Time Distribution
        f.write("\n" + "=" * 100 + "\n")
        f.write("TIME DIFFERENCE DISTRIBUTION\n")
        f.write("=" * 100 + "\n\n")
        for bucket, count in rca_results['time_buckets'].items():
            pct = count / len(df) * 100
            f.write(f"{bucket:15}: {count:4} records ({pct:5.1f}%)\n")
        
        # Root Causes
        f.write("\n\n" + "=" * 100 + "\n")
        f.write("ROOT CAUSES IDENTIFIED\n")
        f.write("=" * 100 + "\n\n")
        
        for i, rc in enumerate(rca_results['root_causes'], 1):
            f.write(f"{i}. [{rc['severity']}] {rc['cause']}\n")
            f.write(f"   Category: {rc['category']}\n")
            f.write(f"   Evidence: {rc['evidence']}\n")
            f.write(f"   Impact: {rc['impact']}\n")
            f.write(f"   Average Delay: {rc.get('avg_delay', 'N/A')}\n")
            if 'max_delay' in rc:
                f.write(f"   Maximum Delay: {rc['max_delay']}\n")
            f.write("-" * 100 + "\n\n")
        
        # Recommendations
        f.write("\n" + "=" * 100 + "\n")
        f.write("RECOMMENDATIONS (PRIORITIZED)\n")
        f.write("=" * 100 + "\n\n")
        
        for i, rec in enumerate(rca_results['recommendations'], 1):
            f.write(f"{rec['priority']} - Recommendation {i}:\n")
            f.write(f"   {rec['recommendation']}\n")
            f.write(f"   Details: {rec['details']}\n")
            f.write(f"   Expected Impact: {rec['impact']}\n")
            f.write("-" * 100 + "\n\n")
        
        # Statistical Details
        f.write("\n" + "=" * 100 + "\n")
        f.write("DETAILED STATISTICS\n")
        f.write("=" * 100 + "\n\n")
        stats = rca_results['patterns']['statistics']
        f.write(f"Mean: {stats['mean_hours']:.2f} hours\n")
        f.write(f"Median: {stats['median_hours']:.2f} hours\n")
        f.write(f"Standard Deviation: {stats['std_hours']:.2f} hours\n")
        f.write(f"Minimum: {stats['min_hours']:.2f} hours\n")
        f.write(f"Maximum: {stats['max_hours']:.2f} hours\n")
        f.write(f"25th Percentile: {stats['p25_hours']:.2f} hours\n")
        f.write(f"75th Percentile: {stats['p75_hours']:.2f} hours\n")
        f.write(f"95th Percentile: {stats['p95_hours']:.2f} hours\n")
        
        # Outliers
        if 'outliers' in rca_results['patterns']:
            f.write("\n\n" + "=" * 100 + "\n")
            f.write("OUTLIER ANALYSIS\n")
            f.write("=" * 100 + "\n\n")
            outliers = rca_results['patterns']['outliers']
            f.write(f"Number of Outliers: {outliers['count']} ({outliers['percentage']}%)\n")
            f.write(f"Threshold: {outliers['threshold_hours']:.2f} hours\n")
            f.write(f"Average Outlier Delay: {outliers['avg_hours']:.2f} hours\n")
            f.write(f"Maximum Delay: {outliers['max_hours']:.2f} hours\n")
        
        # Record-by-Record Analysis
        f.write("\n\n" + "=" * 100 + "\n")
        f.write("DETAILED RECORD-BY-RECORD ANALYSIS\n")
        f.write("=" * 100 + "\n\n")
        
        for analysis in rca_results['detailed_analysis']:
            f.write(f"\nRecord {analysis['record_index']}:\n")
            f.write(f"  Incident Created: {analysis['sys_created_on']}\n")
            f.write(f"  Outage End: {analysis['end_date']}\n")
            f.write(f"  Time Difference: {analysis['time_difference']}\n")
            f.write(f"  Total Hours: {analysis['total_hours']:.2f} ({analysis['total_days']:.2f} days)\n")
            f.write(f"  Classification: {analysis['classification']}\n")
            f.write(f"  Likely Cause: {analysis['likely_cause']}\n")
            f.write(f"  Severity: {analysis['severity']}\n")
            f.write("-" * 100 + "\n")
    
    print(f"\n‚úì Detailed report exported to '{filename}'")


def export_to_excel(rca_results, df, filename='positive_timediff_rca_analysis.xlsx'):
    """Export comprehensive analysis to Excel with multiple sheets."""
    
    with pd.ExcelWriter(filename, engine='openpyxl') as writer:
        # Sheet 1: Summary
        summary_data = {
            'Metric': [
                'Total Sample Records',
                'Average Time Difference (hours)',
                'Average Time Difference (days)',
                'Median Time Difference (hours)',
                'Min Time Difference (hours)',
                'Max Time Difference (hours)',
                'Std Dev (hours)',
                '25th Percentile (hours)',
                '75th Percentile (hours)',
                '95th Percentile (hours)',
                'Root Causes Identified',
                'Critical Severity Issues',
                'High Severity Issues',
                'Recommendations Generated',
                'P1 Priority Actions'
            ],
            'Value': [
                len(df),
                f"{rca_results['patterns']['statistics']['mean_hours']:.2f}",
                f"{rca_results['patterns']['statistics']['mean_hours']/24:.2f}",
                f"{rca_results['patterns']['statistics']['median_hours']:.2f}",
                f"{rca_results['patterns']['statistics']['min_hours']:.2f}",
                f"{rca_results['patterns']['statistics']['max_hours']:.2f}",
                f"{rca_results['patterns']['statistics']['std_hours']:.2f}",
                f"{rca_results['patterns']['statistics']['p25_hours']:.2f}",
                f"{rca_results['patterns']['statistics']['p75_hours']:.2f}",
                f"{rca_results['patterns']['statistics']['p95_hours']:.2f}",
                len(rca_results['root_causes']),
                sum(1 for rc in rca_results['root_causes'] if rc['severity'] == 'Critical'),
                sum(1 for rc in rca_results['root_causes'] if rc['severity'] == 'High'),
                len(rca_results['recommendations']),
                sum(1 for rec in rca_results['recommendations'] if rec['priority'] == 'P1')
            ]
        }
        summary_df = pd.DataFrame(summary_data)
        summary_df.to_excel(writer, sheet_name='Summary', index=False)
        
        # Sheet 2: Time Buckets
        time_buckets_df = pd.DataFrame({
            'Time Range': list(rca_results['time_buckets'].keys()),
            'Count': list(rca_results['time_buckets'].values()),
            'Percentage': [round(v/len(df)*100, 1) for v in rca_results['time_buckets'].values()]
        })
        time_buckets_df.to_excel(writer, sheet_name='Time Distribution', index=False)
        
        # Sheet 3: Root Causes
        root_causes_df = pd.DataFrame(rca_results['root_causes'])
        root_causes_df.to_excel(writer, sheet_name='Root Causes', index=False)
        
        # Sheet 4: Recommendations
        recommendations_df = pd.DataFrame(rca_results['recommendations'])
        recommendations_df.to_excel(writer, sheet_name='Recommendations', index=False)
        
        # Sheet 5: Detailed Analysis
        detailed_df = pd.DataFrame(rca_results['detailed_analysis'])
        detailed_df.to_excel(writer, sheet_name='Detailed Analysis', index=False)
        
        # Sheet 6: Original Sample Data
        df.to_excel(writer, sheet_name='Sample Data', index=False)
        
        # Sheet 7: Outliers (if any)
        if 'outliers' in rca_results['patterns']:
            threshold = rca_results['patterns']['outliers']['threshold_hours']
            outliers_df = df[df['total_hours'] > threshold].copy()
            outliers_df.to_excel(writer, sheet_name='Outliers', index=False)
    
    print(f"\n‚úì Excel report exported to '{filename}'")


# ==================================================================================
# MAIN EXECUTION EXAMPLE
# ==================================================================================

"""
USAGE EXAMPLE:

# 1. Load your sampled data (records with positive time differences)
sample_data_path = 'sample_positive_timediff.xlsx'

# 2. Optional: Load original incidents data for additional context
# original_incidents = pd.read_csv('all_incidents.csv')

# 3. Perform comprehensive RCA
rca_results = perform_positive_timediff_rca(sample_data_path)

# 4. Create visualizations
create_visualizations(pd.read_excel(sample_data_path), rca_results)

# 5. Export detailed text report
export_rca_report(rca_results, pd.read_excel(sample_data_path))

# 6. Export to Excel (multiple sheets with all analysis)
export_to_excel(rca_results, pd.read_excel(sample_data_path))

# 7. Access specific findings
print("\nTop Root Causes:")
for rc in rca_results['root_causes']:
    print(f"- [{rc['severity']}] {rc['cause']}")

print("\nTop Recommendations:")
for rec in rca_results['recommendations'][:3]:
    print(f"- [{rec['priority']}] {rec['recommendation']}")

# 8. Get records by classification
detailed = pd.DataFrame(rca_results['detailed_analysis'])
critical_delays = detailed[detailed['severity'] == 'Critical']
print(f"\nCritical delays: {len(critical_delays)} records")
"""
